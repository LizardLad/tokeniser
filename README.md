# C++ BPE Tokeniser
Given GPT models often use BPE for their input a tokeniser is needed. This implements a simple tokeniser which at the moment can be trained from Wikipedia data. It tends to process data at 60-80MB/s with an Intel i5-13600k. 

### Todo
Add the ability to save and load tokenisers
